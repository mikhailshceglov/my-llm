1) Выбрать текст. Можно взять собрание сочинений Толстого. Можно поискать какие-то тексты (например). Общий объём текста для обучения не менее, чем “Война и Мир”.
2) Построить эмбединг для каждого слова по алгоритму word2vec (L - любое целое от 3 до 6, d = 100, 500, 1000). То есть у вас будет 3 разных эмбединга с длинами 100, 500 и 1000.
3) Обучить полносвязную нейросеть с одним слоем (как на слайде 18). dh = 500 всегда. Сеть должна взять L слов и предсказать одно следующее слово. Вывести качество сети в процессе обучения. Качество измеряется величиной потери.
4) Взять в качестве теста какое-то небольшое предложение. Применить к словам encode из п.2. Передать токены на вход нейросети. Выходные данные - токены. 
5) Сделать всё то же самое, но со стеммингом или токенами. 

Ограничения на лабораторную работу:
1. Необходимо самостоятельно реализовать алгоритм word2vec. Запрещено пользоваться библиотечными функциями, которые его реализуют.
2. Запрещено использовать готовые датасеты с обученными эмбедингами
3. параметр L фиксируется один раз, а три варианта величины d приведут к тому, что у вас будет три разные языковые модели. Необходимо подготовить пример, как каждая из этих моделей реагирует на одно и то же предложение.
4. Необходимо для каждой модели оценить перплексию на заготовленном примере

