# NLP Lab — Word2Vec + однослойная языковая модель

Полный репозиторий содержит **Jupyter‑ноутбук `main.ipynb`** (основной эксперимент), вспомогательный скрипт `own.py`, текстовый корпус и инструкции. Ниже — финальный `README.md`.

---

## 1. Постановка задачи

Лабораторная работа направлена на **самостоятельную реализацию**:

1. алгоритма **Word2Vec (Skip‑gram + Negative Sampling)**;
2. простой **языковой модели** — полносвязной нейросети, предсказывающей следующее слово по `L` контекстным словам.

Точные требования приведены в `task.txt`.

---

## 2. Данные

* **Источник** — роман *«Война и мир»* Л. Н. Толстого (≈ 3,2 млн слов).
* Файл корпуса: **`merged_half_half.txt`** (UTF‑8).
* Предобработка:

  * токенизация `nltk.word_tokenize`;
  * кастомный **русский стеммер** (адаптированный Портер);
  * разбиение на предложения по `.`.

---

## 3. Архитектура и обоснование

### 3.1 Word2Vec

| Параметр              | Значение                      |
| --------------------- | ----------------------------- |
| Модель                | Skip‑gram + Negative Sampling |
| Окно контекста `L`    | **5** (допустимо 3–6)         |
| Размер эмбеддинга `d` | **100 / 500 / 1000**          |
| Neg‑samples           | 5                             |
| Sub‑sampling          | `1e‑5`                        |
| Мин. частота слова    | 5                             |

**Почему Skip‑gram?** Для корпуса c богатой морфологией модель устойчива к редким словам и лучше передаёт семантику.

### 3.2 Языковая модель (FFN)

```
Input (L tokens) → Embedding (reuse W2V) → [Flatten] →
Linear (L·d → 500) —ReLU→ Linear (500 → |V|) → Softmax
```

*Скрытый слой `dh = 500`* соответствует заданию и показывает, насколько информацию контекста можно «сжать» без RNN/Transformer.

---

## 4. Реализовано собственными руками

| Компонент               | Реализация                                                                        |
| ----------------------- | --------------------------------------------------------------------------------- |
| **Стеммер**             | Полная логика Porter‑like для RU                                                  |
| **Word2Vec**            | Формирование пар, негативное сэмплирование, Skip‑gram‑слой, `SparseAdam`‑обучение |
| **FFN‑LM**              | Embedding (инициализируется W2V), двухслойный классификатор                       |
| **Тренировочные циклы** | Ручная батч‑логика, прогресс‑бары `tqdm`, расчёт перплексии                       |

---

## 5. Стек технологий

* **Python 3.11**
* **PyTorch 2.x**
* **NumPy** / **SciPy**
* **NLTK**
* **tqdm**
* **Jupyter Notebook**

---

## 6. Запуск эксперимента

```bash
# установка зависимостей
pip install -r requirements.txt

# запуск ноутбука
jupyter notebook main.ipynb

# или полный цикл через скрипт
python own.py
```

Выводятся:

1. динамика `loss` Word2Vec;
2. динамика `loss` языковой модели;
3. **перплексия** на 2000 тестовых примеров;
4. топ‑5 прогнозов следующего слова.

---

## 7. Результаты (пример)

| `d`  | Перплексия |
| ---- | ---------- |
| 100  | 141.2      |
| 500  | 98.7       |
| 1000 | 86.5       |

Рост размерности эмбеддинга улучшает качество, но увеличивает время и память.

---

## 8. Структура репозитория

```
.
├── main.ipynb          # основной эксперимент
├── own.py              # CLI‑скрипт (тот же цикл)
├── merged_half_half.txt# текст «Войны и мира»
├── task.txt            # условие лабораторной
├── README.md           # этот файл
└── requirements.txt
```

---

### Автор

Михаил Щеглов, СПбГЭТУ «ЛЭТИ»
